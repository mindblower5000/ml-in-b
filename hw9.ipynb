{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создать веб-сервис на Flask через Google Colab или на локальной машине. Научиться отправлять post-запросы с помощью Postman и получать на них ответы. Можно использовать любую из обученных моделей в заданиях ко 2 и 3 занятиям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vgpa6dK_sjd6"
   },
   "source": [
    "## Схема проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3EsTC0PgFSG"
   },
   "source": [
    "**Принципиальные моменты:**\n",
    "*   У страховой компании нет времени \"на подумать\", решение о приеме на страхование или отказе, а также о тарифе должно быть сделано непосредственно в момент обращения клиента за полисом. Соответственно, сервис должен работать в режиме реального времени и давать ответ в течение максимум нескольких секунд.\n",
    "*   Сервис скоринга строится обычно отдельным блоком, а фронт-система взаимодействует с сервисом через API (Application programming interface).\n",
    "*   Под каждый блок, как правило, - свое программное решение.\n",
    "*   Запрос данных из внешних источников может производиться как на уровне фронт-системы, так и на уровне сервиса.\n",
    "*   Один из возможных вариантов: код на Python + Flask + WSGI (взаимодействие между программой на python и веб-сервером) + Web Server. Передача информации производится путем post-запросов и обмена файлами json.\n",
    "*   Для тестирования можно использовать Postman.\n",
    "*   Также бывают варианты:\n",
    "    *   Внедрение моделей в сам код в виде функций;\n",
    "    *   docker + grpc\n",
    "    *   Sparc ML lib + сериализаторы и десериализаторы\n",
    "    *   PMML\n",
    "    *   Flask + Celery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w4V2hYPyAYs9"
   },
   "source": [
    "  ![](https://drive.google.com/uc?export=view&id=1OAOF1M2U14UJWDmeJg2mwo-pgSwyNyzc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0vvTkBa2TfX"
   },
   "source": [
    "Исследование:<br/>\n",
    "  ![](https://drive.google.com/uc?export=view&id=1dnMajOh1EWbjKFIDfvmqYcux1yTdKEJO)\n",
    "<br/>\n",
    "Разработка:<br/>\n",
    "  ![](https://drive.google.com/uc?export=view&id=1DXa3EhgIQXPzfINAETrEhi7lt3NjLR4o)\n",
    "<br/>\n",
    "Тестирование:<br/>\n",
    "  ![](https://drive.google.com/uc?export=view&id=1wQsJ0pipWsjDLfzGOX3n4KMLpn21PNgh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pbavTQ433zj"
   },
   "source": [
    "*   Предобработчики данных - это часть модели ML и должны лежать рядом с моделью, в том числе для синхронизации версий.\n",
    "*   Препроцессинг внешних данных может быть вынесен в отдельные блоки, поддерживающие интеграцию со сторонними api.\n",
    "*   Вновь поступающие данные можно доразмечать и отправлять на дообучение модели.\n",
    "*   Доступ к данным в компании должен быть простым и идентичным у различных пользователей (отделов, департаментов). Лучшая практика - создание DWH. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LAZ15WHl5JO4"
   },
   "source": [
    "*   Модель ML - это программный код.\n",
    "*   Может быть написан на разных языках программирования.\n",
    "*   Сейчас чаще код пишут на Python, хотя \"под капотом\" библиотек может быть и Java, и C.\n",
    "*   Для передачи между участниками процесса может быть выполнена сериализация модели (перевод в последовательность битов - json или xml), используются библиотеки pickle, drill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XBG3_nd46p73"
   },
   "source": [
    "**Общие концепции сервинга:**\n",
    "*   Ввести идентификатор модели\n",
    "*   В папке хранить сериализованную модель и веса коэффициентов\n",
    "*   Загружать модель в память при старте сервиса\n",
    "*   Обрабатывать входящие запросы через http и вызывать модель\n",
    "*   Для картинок можно использовать grpc\n",
    "*   Фичи, как правило, передаются в теле запроса post\n",
    "*   На каждую модель не нужно писать отдельный сервис, а подгружать модель из папки\n",
    "*   При сохранении модель обязательно сопровождать документацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5z3-vxNngRLY"
   },
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I3qRhEzYAZE_"
   },
   "source": [
    "JSON (JavaScript Object Notation) - простой текстовый формат обмена данными, он основан на подмножестве языка программирования JavaScript.<br/>\n",
    "Например, строка из нашего датасета выглядела бы следующим образом:<br/>\n",
    "```\n",
    "{\n",
    "\"ID\": 1,\n",
    "\"Exposure\": 0.583,\n",
    "\"RecordBeg\": \"2004-06-01\",\n",
    "\"RecordEnd\": \"\",\n",
    "\"DrivAge\": 55,\n",
    "\"Gender\": \"Female\",\n",
    "...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_KCCtAZqgbI6"
   },
   "source": [
    "## При внедрении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwDoeEWdRZUv"
   },
   "source": [
    "**При внедрении необходимо сделать:**\n",
    "*   Определить формат json'а, в котором данные будут приниматься сервисом и отправляться обратно.\n",
    "*   Определить ip-адрес и порт, на который будут поступать данные.\n",
    "*   Создать во Flask необходимые роуты:<br/>\n",
    "    `@app.route('/predict_example', method='POST')`<br/>\n",
    "    `def predict_example():`\n",
    "*   Перенести во Flask все функции преобразования данных,\n",
    "    *   формат данных, приходящих от фронт-системы, может отличаться от формата исторических данных, использовавшихся при построении модели; в результате преобразований данные на вход модели должны поступить ровно в том виде, в каком была обучена модель.\n",
    "*   Загрузить обученные модели.\n",
    "*   Настроить логирование, запись котировок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1c-2EUcjRZQp"
   },
   "source": [
    "**Особенности:**\n",
    "*   Библиотека H2O использует виртуальную Java-машину:\n",
    "    *   ее нужно инициализировать один раз, а не поднимать заново для каждого расчета;\n",
    "    *   вручную выделить под нее отдельный порт и указать размер используемой памяти;\n",
    "    *   не создавать каждый раз заново H2O-Frame, а записывать в единожды подготовленный.\n",
    "*   Необходимо удостовериться, что на всех этапах сервис отрабатывает корректно; например, можно иметь заготовленный массив котировок с заранее известными ответами.\n",
    "*   Необходимо провести нагрузочное тестирование и удостовериться, что сервис справляется с нагрузкой.\n",
    "*   Для согласованности версий Python, Java при переносе на другие серверы имеет смысл использовать докеры.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "koF_iLbj89XC"
   },
   "source": [
    "## Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDCtyzx2_-OV"
   },
   "source": [
    "Google Colab provides a virtual machine so we cannot access the localhost as we do on our local machine when running a local web server. What we can do is expose it to a public URL using ngrok.\n",
    "https://medium.com/@kshitijvijay271199/flask-on-google-colab-f6525986797b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47322,
     "status": "ok",
     "timestamp": 1581782327067,
     "user": {
      "displayName": "Андрей Андоскин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBawjEve_gbWzJ4duKha8cVAVDvdr-Xh1CdtWkb=s64",
      "userId": "13645431636972563579"
     },
     "user_tz": -180
    },
    "id": "22vvlI1qjJjQ",
    "outputId": "7cb3576b-00bb-49a9-ae48-d4bcd3a75a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTzkbmHWkvP0"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/My Drive/ds_in_prodact/freMPL-R.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5667,
     "status": "ok",
     "timestamp": 1581782167486,
     "user": {
      "displayName": "Андрей Андоскин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBawjEve_gbWzJ4duKha8cVAVDvdr-Xh1CdtWkb=s64",
      "userId": "13645431636972563579"
     },
     "user_tz": -180
    },
    "id": "2laQ3An4_06O",
    "outputId": "5206e45a-c001-4a8f-a01a-6cec700b663d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask-ngrok\n",
      "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.21.0)\n",
      "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.1)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
      "Installing collected packages: flask-ngrok\n",
      "Successfully installed flask-ngrok-0.0.25\n"
     ]
    }
   ],
   "source": [
    "!pip install flask-ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KaWqHrF2865_"
   },
   "outputs": [],
   "source": [
    "from flask_ngrok import run_with_ngrok\n",
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 96422,
     "status": "ok",
     "timestamp": 1581782275005,
     "user": {
      "displayName": "Андрей Андоскин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBawjEve_gbWzJ4duKha8cVAVDvdr-Xh1CdtWkb=s64",
      "userId": "13645431636972563579"
     },
     "user_tz": -180
    },
    "id": "YKvAxhnG9f6m",
    "outputId": "4d9066fd-d246-475f-966e-c110ad4441a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Running on http://9ace4b00.ngrok.io\n",
      " * Traffic stats available on http://127.0.0.1:4040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [15/Feb/2020 15:56:52] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [15/Feb/2020 15:56:52] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [15/Feb/2020 15:56:55] \"\u001b[37mGET /a HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Пробный запуск Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "run_with_ngrok(app)  # Start ngrok when app is run\n",
    "\n",
    "@app.route(\"/a\")\n",
    "def hello():\n",
    "    return \"Hello World!\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-_2eEnWyaJp"
   },
   "source": [
    "В качестве примера используем обученную обобщенную линейную модель (GLM) и библиотеку H2O."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C185KYSCypLn"
   },
   "source": [
    "Преобразование данных и обучение модели были проведены на втором занятии. Сейчас воспользуемся готовым результатом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25687,
     "status": "ok",
     "timestamp": 1581782559437,
     "user": {
      "displayName": "Андрей Андоскин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBawjEve_gbWzJ4duKha8cVAVDvdr-Xh1CdtWkb=s64",
      "userId": "13645431636972563579"
     },
     "user_tz": -180
    },
    "id": "OEPe9Xk99f_U",
    "outputId": "96db89e6-3258-472e-b883-18672ecc25c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h2o\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/f5/23c0d33d0b4880cbb09327ba8bc0072d366bc80eada9122bd6758ef1d365/h2o-3.28.0.3.tar.gz (126.2MB)\n",
      "\u001b[K     |████████████████████████████████| 126.2MB 77kB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from h2o) (2.21.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from h2o) (0.8.6)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from h2o) (0.16.0)\n",
      "Collecting colorama>=0.3.8\n",
      "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->h2o) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->h2o) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->h2o) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->h2o) (2019.11.28)\n",
      "Building wheels for collected packages: h2o\n",
      "  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for h2o: filename=h2o-3.28.0.3-py2.py3-none-any.whl size=126303006 sha256=0f47c1a66247156601048b4947c64dc2f04618ccea40f653ef4e333239edcdcd\n",
      "  Stored in directory: /root/.cache/pip/wheels/fa/7c/ce/95ae52b4d3f1b14a27c3c961c1f94635aee841ab1eec3aeeca\n",
      "Successfully built h2o\n",
      "Installing collected packages: colorama, h2o\n",
      "Successfully installed colorama-0.4.3 h2o-3.28.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9887,
     "status": "ok",
     "timestamp": 1581783818392,
     "user": {
      "displayName": "Андрей Андоскин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBawjEve_gbWzJ4duKha8cVAVDvdr-Xh1CdtWkb=s64",
      "userId": "13645431636972563579"
     },
     "user_tz": -180
    },
    "id": "54UX4rlg9gDG",
    "outputId": "95cbc39d-393b-474f-b2e7-095c20c0872d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"11.0.6\" 2020-01-14; OpenJDK Runtime Environment (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1); OpenJDK 64-Bit Server VM (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1, mixed mode, sharing)\n",
      "  Starting server from /usr/local/lib/python3.6/dist-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpobcobvsb\n",
      "  JVM stdout: /tmp/tmpobcobvsb/h2o_unknownUser_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpobcobvsb/h2o_unknownUser_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>03 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.28.0.3</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>9 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_unknownUser_ko75k5</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>2</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>2</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>{'http': None, 'https': None}</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.9 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------------------------------\n",
       "H2O cluster uptime:         03 secs\n",
       "H2O cluster timezone:       Etc/UTC\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.28.0.3\n",
       "H2O cluster version age:    9 days\n",
       "H2O cluster name:           H2O_from_python_unknownUser_ko75k5\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3 Gb\n",
       "H2O cluster total cores:    2\n",
       "H2O cluster allowed cores:  2\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:       {'http': None, 'https': None}\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python version:             3.6.9 final\n",
       "--------------------------  ------------------------------------------------------------------"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24JU2uFVT-Sk"
   },
   "outputs": [],
   "source": [
    "# Загружаем обученные модели\n",
    "\n",
    "model_glm_binomial = h2o.load_model('/content/drive/My Drive/ds_in_prodact/GLM_model_python_1581783997770_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 125825,
     "status": "ok",
     "timestamp": 1581784938133,
     "user": {
      "displayName": "Андрей Андоскин",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBawjEve_gbWzJ4duKha8cVAVDvdr-Xh1CdtWkb=s64",
      "userId": "13645431636972563579"
     },
     "user_tz": -180
    },
    "id": "pmii6qPeGInX",
    "outputId": "5c268305-a77e-4f44-f67e-956b845e652d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Running on http://53779676.ngrok.io\n",
      " * Traffic stats available on http://127.0.0.1:4040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [15/Feb/2020 16:41:21] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [15/Feb/2020 16:41:21] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Feb/2020 16:41:22] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [15/Feb/2020 16:41:22] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Feb/2020 16:41:31] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [15/Feb/2020 16:41:31] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Запуск Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "run_with_ngrok(app)  # Start ngrok when app is run\n",
    "\n",
    "@app.route('/predict', methods=['GET', 'POST'])\n",
    "def predict3():\n",
    "\n",
    "    try:\n",
    "\n",
    "      json_input = request.json\n",
    "\n",
    "      ID = json_input[\"ID\"]\n",
    "      LicAge = json_input[\"LicAge\"]\n",
    "      Gender = map_for_dict_Gender(json_input[\"Gender\"])\n",
    "      MariStat = map_for_dict_MariStat(json_input[\"MariStat\"])\n",
    "      DrivAge = json_input[\"DrivAge\"]\n",
    "      HasKmLimit = json_input[\"HasKmLimit\"]\n",
    "      BonusMalus = json_input[\"BonusMalus\"]\n",
    "      OutUseNb = json_input[\"OutUseNb\"]\n",
    "      RiskArea = json_input[\"RiskArea\"]\n",
    "      VehUsg_Private = f_VehUsage_Private(json_input[\"VehUsage\"])\n",
    "      VehUsg_Private_trip_to_office = f_VehUsage_Private_trip_to_office(json_input[\"VehUsage\"])\n",
    "      VehUsg_Professional = f_VehUsage_Professional(json_input[\"VehUsage\"])\n",
    "      VehUsg_Professional_run = f_VehUsage_Professional_run(json_input[\"VehUsage\"])\n",
    "      CSP1 = 0\n",
    "      CSP2 = 0\n",
    "      CSP3 = 0\n",
    "      CSP6 = 0\n",
    "      CSP7 = 0\n",
    "      CSP20 = 0\n",
    "      CSP21 = 0\n",
    "      CSP22 = 0\n",
    "      CSP26 = 0\n",
    "      CSP37 = 0\n",
    "      CSP40 = 0\n",
    "      CSP42 = 0\n",
    "      CSP46 = 0\n",
    "      CSP47 = 0\n",
    "      CSP48 = 0\n",
    "      CSP49 = 0\n",
    "      CSP50 = 0\n",
    "      CSP55 = 0\n",
    "      CSP56 = 0\n",
    "      CSP57 = 0\n",
    "      CSP60 = 0\n",
    "      CSP65 = 0\n",
    "      CSP66 = 0\n",
    "\n",
    "      hf = return_NewH2o_Frame()\n",
    "\n",
    "      hf[0, 'LicAge'] = LicAge\n",
    "      hf[0, 'Gender'] = Gender\n",
    "      hf[0, 'MariStat'] = MariStat\n",
    "      hf[0, 'DrivAge'] = DrivAge\n",
    "      hf[0, 'HasKmLimit'] = HasKmLimit\n",
    "      hf[0, 'BonusMalus'] = BonusMalus\n",
    "      hf[0, 'OutUseNb'] = OutUseNb\n",
    "      hf[0, 'RiskArea'] = RiskArea\n",
    "      hf[0, 'VehUsg_Private'] = VehUsg_Private\n",
    "      hf[0, 'VehUsg_Private+trip to office'] = VehUsg_Private_trip_to_office\n",
    "      hf[0, 'VehUsg_Professional'] = VehUsg_Professional\n",
    "      hf[0, 'VehUsg_Professional run'] = VehUsg_Professional_run\n",
    "      hf[0, 'CSP1'] = CSP1\n",
    "      hf[0, 'CSP2'] = CSP2\n",
    "      hf[0, 'CSP3'] = CSP3\n",
    "      hf[0, 'CSP6'] = CSP6\n",
    "      hf[0, 'CSP7'] = CSP7\n",
    "      hf[0, 'CSP20'] = CSP20\n",
    "      hf[0, 'CSP21'] = CSP21\n",
    "      hf[0, 'CSP22'] = CSP22\n",
    "      hf[0, 'CSP26'] = CSP26\n",
    "      hf[0, 'CSP37'] = CSP37\n",
    "      hf[0, 'CSP40'] = CSP40\n",
    "      hf[0, 'CSP42'] = CSP42\n",
    "      hf[0, 'CSP46'] = CSP46\n",
    "      hf[0, 'CSP47'] = CSP47\n",
    "      hf[0, 'CSP48'] = CSP48\n",
    "      hf[0, 'CSP49'] = CSP49\n",
    "      hf[0, 'CSP50'] = CSP50\n",
    "      hf[0, 'CSP55'] = CSP55\n",
    "      hf[0, 'CSP56'] = CSP56\n",
    "      hf[0, 'CSP57'] = CSP57\n",
    "      hf[0, 'CSP60'] = CSP60\n",
    "      hf[0, 'CSP65'] = CSP65\n",
    "      hf[0, 'CSP66'] = CSP66\n",
    "\n",
    "      prediction_Binomial = model_glm_binomial.predict(hf)\n",
    "      value_Binomial  = prediction_Binomial.as_data_frame()['predict'][0]\n",
    "\n",
    "      return jsonify({'ID':ID, 'value_Binomial':value_Binomial}) \n",
    "    \n",
    "    except:\n",
    "      \n",
    "      return \"Error\"\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I3cg93cYxXiR"
   },
   "source": [
    "##Postman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6agfYOY3xajD"
   },
   "source": [
    "Для тестирования сервиса через API удобно использовать программу Postman https://www.postman.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WzGEZzEDxaui"
   },
   "source": [
    "Создаем тестовый json со значениями параметров в соответствии с обученной моделью<br/>\n",
    "```\n",
    "{\n",
    "\"ID\":1,\n",
    "\"LicAge\":468,\n",
    "\"RecordBeg\":\"2004-01-01\",\n",
    "\"RecordEnd\":\"\",\n",
    "\"VehAge\":\"\",\n",
    "\"Gender\":\"Male\",\n",
    "\"MariStat\":\"Other\",\n",
    "\"SocioCateg\":\"CSP50\",\n",
    "\"VehUsage\":\"Private\",\n",
    "\"DrivAge\":67,\n",
    "\"HasKmLimit\":0,\n",
    "\"BonusMalus\":50,\n",
    "\"OutUseNb\":0,\n",
    "\"RiskArea\":0\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5eLuKx5zLSU"
   },
   "source": [
    "Запускаем Postman, выбираем VALUE = 'application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3RFM8WCdxa-T"
   },
   "source": [
    "  ![](https://drive.google.com/uc?export=view&id=16z29a8xAmRrRXaEb5mpjufi458W5WAsZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cvn-QpAHzHvl"
   },
   "source": [
    "Формируем тестовый post-запрос"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iIU_k0mCzcxd"
   },
   "source": [
    "  ![](https://drive.google.com/uc?export=view&id=1enCft4k135BYAH01UgAsSX8Tj7LxaS0b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWCoNkhbzqVN"
   },
   "source": [
    "Отправляем post-запрос на url Flask (см. выше)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ghpc-Bbyzc2f"
   },
   "source": [
    "  ![](https://drive.google.com/uc?export=view&id=143XO4e9NfH46APwPLwILDFcNmM3wtACj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hWfyud1ozc7Z"
   },
   "source": [
    "Получаем ответ от сервиса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sk4uF5R20aOb"
   },
   "source": [
    "  ![](https://drive.google.com/uc?export=view&id=1ZjRp5Pys_Bg2m1KM4iWtczCdIxx_zCIv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKw7VwLy0y8Z"
   },
   "source": [
    "При обучении новых моделей (даже на той же тренировочной выборке) коэффициенты и, соответственно, прогнозные значения могут отличаться от приведенных выше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJXHGUhV1XEv"
   },
   "source": [
    "##Дополнительно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2oQhEluw840X"
   },
   "source": [
    "*   Логирование\n",
    "    *   Пришел запрос\n",
    "    *   Что пришло\n",
    "    *   Ушел ответ\n",
    "    *   Что ушло\n",
    "    *   Средства зависят от системы логирования, ELK stack (elasticsearch, logstash, kibana)\n",
    "*   requirements.txt\n",
    "*   pip freeze\n",
    "   ![](https://drive.google.com/uc?export=view&id=1b2LKW_kYSTUPUcLV3i3KqWmGqzv_hGh5)\n",
    "*   Многопоточность\n",
    "    *   Изначально веб-серверы на Python работают в developer mode\n",
    "    *   Из-за GIL (python global interpreter lock) активен только один поток (в один момент времени может активно обрабатываться только один запрос)\n",
    "    *   Flask, Django production mode (WSGI - программный интерфейс для питоновских программ)\n",
    "    *   Веб-сервер сам параллелит запросы\n",
    "*   Сокращение времени вывода модели в прод:\n",
    "    *   Data Scientist должен предоставить скрипт, который легко встроить в веб-вервис\n",
    "    *   Сервинг - процесс перевода модели в продукцию, в рамках которого происходит сохранение функций и обработчиков в файл\n",
    "*   Контейнеризация:\n",
    "    *   Kubernetes + docker\n",
    "    *   Масштабирование приложений, не имеющих изменяемого состояния\n",
    "    *   Запросы распределяются на различные приложения\n",
    "*   Серьезный прод - grpc:\n",
    "    *   grpc в 2 слоя\n",
    "    *   первый слой: grpc + модели, поднимаются контейнеры с моделями, которые могут принимать запросы на инференс\n",
    "    *   второй слой: поднимается прокси на чем-нибудь асинхронном (grpc-gateway, go, aiohttp), у которого снаружи http, а сам он делает грс вызовы по grpc\n",
    "    *   плюсы такой схемы: можно независимо скейлить модели и gateway, делатб AB-тесты через canary deployment и sticky routing для пользователя\n",
    "    *   grpc позволяет делать bidirectional streaming, когда можно принять за определенное временное окно пачку запросов, сложить их в батч и сделать по ним инференс, а потом поштучно отстсрелить\n",
    "    *   если модель делает запросы к внешним источникам, то хороший вариант - aiohttp, а инференс можно сделать через 'run_in_executor' в пул процессов, если сериализация контекста не является затратной "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Схема проекта\n",
    "\n",
    "**Принципиальные моменты:**\n",
    "*   У страховой компании нет времени \"на подумать\", решение о приеме на страхование или отказе, а также о тарифе должно быть сделано непосредственно в момент обращения клиента за полисом. Соответственно, сервис должен работать в режиме реального времени и давать ответ в течение максимум нескольких секунд.\n",
    "*   Сервис скоринга строится обычно отдельным блоком, а фронт-система взаимодействует с сервисом через API (Application programming interface).\n",
    "*   Под каждый блок, как правило, - свое программное решение.\n",
    "*   Запрос данных из внешних источников может производиться как на уровне фронт-системы, так и на уровне сервиса.\n",
    "*   Один из возможных вариантов: код на Python + Flask + WSGI (взаимодействие между программой на python и веб-сервером) + Web Server. Передача информации производится путем post-запросов и обмена файлами json.\n",
    "*   Для тестирования можно использовать Postman.\n",
    "*   Также бывают варианты:\n",
    "    *   Внедрение моделей в сам код в виде функций;\n",
    "    *   docker + grpc\n",
    "    *   Sparc ML lib + сериализаторы и десериализаторы\n",
    "    *   PMML\n",
    "    *   Flask + Celery\n",
    "\n",
    "  ![](https://drive.google.com/uc?export=view&id=1OAOF1M2U14UJWDmeJg2mwo-pgSwyNyzc)\n",
    "\n",
    "Исследование:<br/>\n",
    "  ![](https://drive.google.com/uc?export=view&id=1dnMajOh1EWbjKFIDfvmqYcux1yTdKEJO)\n",
    "<br/>\n",
    "Разработка:<br/>\n",
    "  ![](https://drive.google.com/uc?export=view&id=1DXa3EhgIQXPzfINAETrEhi7lt3NjLR4o)\n",
    "<br/>\n",
    "Тестирование:<br/>\n",
    "  ![](https://drive.google.com/uc?export=view&id=1wQsJ0pipWsjDLfzGOX3n4KMLpn21PNgh)\n",
    "\n",
    "*   Предобработчики данных - это часть модели ML и должны лежать рядом с моделью, в том числе для синхронизации версий.\n",
    "*   Препроцессинг внешних данных может быть вынесен в отдельные блоки, поддерживающие интеграцию со сторонними api.\n",
    "*   Вновь поступающие данные можно доразмечать и отправлять на дообучение модели.\n",
    "*   Доступ к данным в компании должен быть простым и идентичным у различных пользователей (отделов, департаментов). Лучшая практика - создание DWH. \n",
    "\n",
    "*   Модель ML - это программный код.\n",
    "*   Может быть написан на разных языках программирования.\n",
    "*   Сейчас чаще код пишут на Python, хотя \"под капотом\" библиотек может быть и Java, и C.\n",
    "*   Для передачи между участниками процесса может быть выполнена сериализация модели (перевод в последовательность битов - json или xml), используются библиотеки pickle, drill\n",
    "\n",
    "**Общие концепции сервинга:**\n",
    "*   Ввести идентификатор модели\n",
    "*   В папке хранить сериализованную модель и веса коэффициентов\n",
    "*   Загружать модель в память при старте сервиса\n",
    "*   Обрабатывать входящие запросы через http и вызывать модель\n",
    "*   Для картинок можно использовать grpc\n",
    "*   Фичи, как правило, передаются в теле запроса post\n",
    "*   На каждую модель не нужно писать отдельный сервис, а подгружать модель из папки\n",
    "*   При сохранении модель обязательно сопровождать документацией\n",
    "\n",
    "## JSON\n",
    "\n",
    "JSON (JavaScript Object Notation) - простой текстовый формат обмена данными, он основан на подмножестве языка программирования JavaScript.<br/>\n",
    "Например, строка из нашего датасета выглядела бы следующим образом:<br/>\n",
    "```\n",
    "{\n",
    "\"ID\": 1,\n",
    "\"Exposure\": 0.583,\n",
    "\"RecordBeg\": \"2004-06-01\",\n",
    "\"RecordEnd\": \"\",\n",
    "\"DrivAge\": 55,\n",
    "\"Gender\": \"Female\",\n",
    "...\n",
    "}\n",
    "```\n",
    "\n",
    "## При внедрении\n",
    "\n",
    "**При внедрении необходимо сделать:**\n",
    "*   Определить формат json'а, в котором данные будут приниматься сервисом и отправляться обратно.\n",
    "*   Определить ip-адрес и порт, на который будут поступать данные.\n",
    "*   Создать во Flask необходимые роуты:<br/>\n",
    "    `@app.route('/predict_example', method='POST')`<br/>\n",
    "    `def predict_example():`\n",
    "*   Перенести во Flask все функции преобразования данных,\n",
    "    *   формат данных, приходящих от фронт-системы, может отличаться от формата исторических данных, использовавшихся при построении модели; в результате преобразований данные на вход модели должны поступить ровно в том виде, в каком была обучена модель.\n",
    "*   Загрузить обученные модели.\n",
    "*   Настроить логирование, запись котировок.\n",
    "\n",
    "**Особенности:**\n",
    "*   Библиотека H2O использует виртуальную Java-машину:\n",
    "    *   ее нужно инициализировать один раз, а не поднимать заново для каждого расчета;\n",
    "    *   вручную выделить под нее отдельный порт и указать размер используемой памяти;\n",
    "    *   не создавать каждый раз заново H2O-Frame, а записывать в единожды подготовленный.\n",
    "*   Необходимо удостовериться, что на всех этапах сервис отрабатывает корректно; например, можно иметь заготовленный массив котировок с заранее известными ответами.\n",
    "*   Необходимо провести нагрузочное тестирование и удостовериться, что сервис справляется с нагрузкой.\n",
    "*   Для согласованности версий Python, Java при переносе на другие серверы имеет смысл использовать докеры.\n",
    "\n",
    "\n",
    "\n",
    "## Flask\n",
    "\n",
    "Google Colab provides a virtual machine so we cannot access the localhost as we do on our local machine when running a local web server. What we can do is expose it to a public URL using ngrok.\n",
    "https://medium.com/@kshitijvijay271199/flask-on-google-colab-f6525986797b\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "df = pd.read_csv('/content/drive/My Drive/ds_in_prodact/freMPL-R.csv', low_memory=False)\n",
    "\n",
    "!pip install flask-ngrok\n",
    "\n",
    "from flask_ngrok import run_with_ngrok\n",
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "\n",
    "# Пробный запуск Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "run_with_ngrok(app)  # Start ngrok when app is run\n",
    "\n",
    "@app.route(\"/a\")\n",
    "def hello():\n",
    "    return \"Hello World!\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "\n",
    "В качестве примера используем обученную обобщенную линейную модель (GLM) и библиотеку H2O.\n",
    "\n",
    "Преобразование данных и обучение модели были проведены на втором занятии. Сейчас воспользуемся готовым результатом.\n",
    "\n",
    "!pip install h2o\n",
    "\n",
    "import h2o\n",
    "h2o.init()\n",
    "\n",
    "# Загружаем обученные модели\n",
    "\n",
    "model_glm_binomial = h2o.load_model('/content/drive/My Drive/ds_in_prodact/GLM_model_python_1581783997770_1')\n",
    "\n",
    "# Запуск Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "run_with_ngrok(app)  # Start ngrok when app is run\n",
    "\n",
    "@app.route('/predict', methods=['GET', 'POST'])\n",
    "def predict3():\n",
    "\n",
    "    try:\n",
    "\n",
    "      json_input = request.json\n",
    "\n",
    "      ID = json_input[\"ID\"]\n",
    "      LicAge = json_input[\"LicAge\"]\n",
    "      Gender = map_for_dict_Gender(json_input[\"Gender\"])\n",
    "      MariStat = map_for_dict_MariStat(json_input[\"MariStat\"])\n",
    "      DrivAge = json_input[\"DrivAge\"]\n",
    "      HasKmLimit = json_input[\"HasKmLimit\"]\n",
    "      BonusMalus = json_input[\"BonusMalus\"]\n",
    "      OutUseNb = json_input[\"OutUseNb\"]\n",
    "      RiskArea = json_input[\"RiskArea\"]\n",
    "      VehUsg_Private = f_VehUsage_Private(json_input[\"VehUsage\"])\n",
    "      VehUsg_Private_trip_to_office = f_VehUsage_Private_trip_to_office(json_input[\"VehUsage\"])\n",
    "      VehUsg_Professional = f_VehUsage_Professional(json_input[\"VehUsage\"])\n",
    "      VehUsg_Professional_run = f_VehUsage_Professional_run(json_input[\"VehUsage\"])\n",
    "      CSP1 = 0\n",
    "      CSP2 = 0\n",
    "      CSP3 = 0\n",
    "      CSP6 = 0\n",
    "      CSP7 = 0\n",
    "      CSP20 = 0\n",
    "      CSP21 = 0\n",
    "      CSP22 = 0\n",
    "      CSP26 = 0\n",
    "      CSP37 = 0\n",
    "      CSP40 = 0\n",
    "      CSP42 = 0\n",
    "      CSP46 = 0\n",
    "      CSP47 = 0\n",
    "      CSP48 = 0\n",
    "      CSP49 = 0\n",
    "      CSP50 = 0\n",
    "      CSP55 = 0\n",
    "      CSP56 = 0\n",
    "      CSP57 = 0\n",
    "      CSP60 = 0\n",
    "      CSP65 = 0\n",
    "      CSP66 = 0\n",
    "\n",
    "      hf = return_NewH2o_Frame()\n",
    "\n",
    "      hf[0, 'LicAge'] = LicAge\n",
    "      hf[0, 'Gender'] = Gender\n",
    "      hf[0, 'MariStat'] = MariStat\n",
    "      hf[0, 'DrivAge'] = DrivAge\n",
    "      hf[0, 'HasKmLimit'] = HasKmLimit\n",
    "      hf[0, 'BonusMalus'] = BonusMalus\n",
    "      hf[0, 'OutUseNb'] = OutUseNb\n",
    "      hf[0, 'RiskArea'] = RiskArea\n",
    "      hf[0, 'VehUsg_Private'] = VehUsg_Private\n",
    "      hf[0, 'VehUsg_Private+trip to office'] = VehUsg_Private_trip_to_office\n",
    "      hf[0, 'VehUsg_Professional'] = VehUsg_Professional\n",
    "      hf[0, 'VehUsg_Professional run'] = VehUsg_Professional_run\n",
    "      hf[0, 'CSP1'] = CSP1\n",
    "      hf[0, 'CSP2'] = CSP2\n",
    "      hf[0, 'CSP3'] = CSP3\n",
    "      hf[0, 'CSP6'] = CSP6\n",
    "      hf[0, 'CSP7'] = CSP7\n",
    "      hf[0, 'CSP20'] = CSP20\n",
    "      hf[0, 'CSP21'] = CSP21\n",
    "      hf[0, 'CSP22'] = CSP22\n",
    "      hf[0, 'CSP26'] = CSP26\n",
    "      hf[0, 'CSP37'] = CSP37\n",
    "      hf[0, 'CSP40'] = CSP40\n",
    "      hf[0, 'CSP42'] = CSP42\n",
    "      hf[0, 'CSP46'] = CSP46\n",
    "      hf[0, 'CSP47'] = CSP47\n",
    "      hf[0, 'CSP48'] = CSP48\n",
    "      hf[0, 'CSP49'] = CSP49\n",
    "      hf[0, 'CSP50'] = CSP50\n",
    "      hf[0, 'CSP55'] = CSP55\n",
    "      hf[0, 'CSP56'] = CSP56\n",
    "      hf[0, 'CSP57'] = CSP57\n",
    "      hf[0, 'CSP60'] = CSP60\n",
    "      hf[0, 'CSP65'] = CSP65\n",
    "      hf[0, 'CSP66'] = CSP66\n",
    "\n",
    "      prediction_Binomial = model_glm_binomial.predict(hf)\n",
    "      value_Binomial  = prediction_Binomial.as_data_frame()['predict'][0]\n",
    "\n",
    "      return jsonify({'ID':ID, 'value_Binomial':value_Binomial}) \n",
    "    \n",
    "    except:\n",
    "      \n",
    "      return \"Error\"\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "\n",
    "##Postman\n",
    "\n",
    "Для тестирования сервиса через API удобно использовать программу Postman https://www.postman.com/\n",
    "\n",
    "Создаем тестовый json со значениями параметров в соответствии с обученной моделью<br/>\n",
    "```\n",
    "{\n",
    "\"ID\":1,\n",
    "\"LicAge\":468,\n",
    "\"RecordBeg\":\"2004-01-01\",\n",
    "\"RecordEnd\":\"\",\n",
    "\"VehAge\":\"\",\n",
    "\"Gender\":\"Male\",\n",
    "\"MariStat\":\"Other\",\n",
    "\"SocioCateg\":\"CSP50\",\n",
    "\"VehUsage\":\"Private\",\n",
    "\"DrivAge\":67,\n",
    "\"HasKmLimit\":0,\n",
    "\"BonusMalus\":50,\n",
    "\"OutUseNb\":0,\n",
    "\"RiskArea\":0\n",
    "}\n",
    "```\n",
    "\n",
    "Запускаем Postman, выбираем VALUE = 'application/json'\n",
    "\n",
    "  ![](https://drive.google.com/uc?export=view&id=16z29a8xAmRrRXaEb5mpjufi458W5WAsZ)\n",
    "\n",
    "Формируем тестовый post-запрос\n",
    "\n",
    "  ![](https://drive.google.com/uc?export=view&id=1enCft4k135BYAH01UgAsSX8Tj7LxaS0b)\n",
    "\n",
    "Отправляем post-запрос на url Flask (см. выше)\n",
    "\n",
    "  ![](https://drive.google.com/uc?export=view&id=143XO4e9NfH46APwPLwILDFcNmM3wtACj)\n",
    "\n",
    "Получаем ответ от сервиса\n",
    "\n",
    "  ![](https://drive.google.com/uc?export=view&id=1ZjRp5Pys_Bg2m1KM4iWtczCdIxx_zCIv)\n",
    "\n",
    "При обучении новых моделей (даже на той же тренировочной выборке) коэффициенты и, соответственно, прогнозные значения могут отличаться от приведенных выше\n",
    "\n",
    "##Дополнительно\n",
    "\n",
    "*   Логирование\n",
    "    *   Пришел запрос\n",
    "    *   Что пришло\n",
    "    *   Ушел ответ\n",
    "    *   Что ушло\n",
    "    *   Средства зависят от системы логирования, ELK stack (elasticsearch, logstash, kibana)\n",
    "*   requirements.txt\n",
    "*   pip freeze\n",
    "   ![](https://drive.google.com/uc?export=view&id=1b2LKW_kYSTUPUcLV3i3KqWmGqzv_hGh5)\n",
    "*   Многопоточность\n",
    "    *   Изначально веб-серверы на Python работают в developer mode\n",
    "    *   Из-за GIL (python global interpreter lock) активен только один поток (в один момент времени может активно обрабатываться только один запрос)\n",
    "    *   Flask, Django production mode (WSGI - программный интерфейс для питоновских программ)\n",
    "    *   Веб-сервер сам параллелит запросы\n",
    "*   Сокращение времени вывода модели в прод:\n",
    "    *   Data Scientist должен предоставить скрипт, который легко встроить в веб-вервис\n",
    "    *   Сервинг - процесс перевода модели в продукцию, в рамках которого происходит сохранение функций и обработчиков в файл\n",
    "*   Контейнеризация:\n",
    "    *   Kubernetes + docker\n",
    "    *   Масштабирование приложений, не имеющих изменяемого состояния\n",
    "    *   Запросы распределяются на различные приложения\n",
    "*   Серьезный прод - grpc:\n",
    "    *   grpc в 2 слоя\n",
    "    *   первый слой: grpc + модели, поднимаются контейнеры с моделями, которые могут принимать запросы на инференс\n",
    "    *   второй слой: поднимается прокси на чем-нибудь асинхронном (grpc-gateway, go, aiohttp), у которого снаружи http, а сам он делает грс вызовы по grpc\n",
    "    *   плюсы такой схемы: можно независимо скейлить модели и gateway, делатб AB-тесты через canary deployment и sticky routing для пользователя\n",
    "    *   grpc позволяет делать bidirectional streaming, когда можно принять за определенное временное окно пачку запросов, сложить их в батч и сделать по ним инференс, а потом поштучно отстсрелить\n",
    "    *   если модель делает запросы к внешним источникам, то хороший вариант - aiohttp, а инференс можно сделать через 'run_in_executor' в пул процессов, если сериализация контекста не является затратной "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
